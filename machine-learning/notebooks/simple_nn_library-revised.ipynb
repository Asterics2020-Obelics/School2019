{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooking a simple neural network library - revised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients\n",
    "\n",
    "- `numpy`\n",
    "- [a loss function](#Loss-function)\n",
    "- [some layers](#Layers)\n",
    "- [a neural net](#Neural-network)\n",
    "- [an optimizer](#Optimizer)\n",
    "- [a batch data provider](#Batch-generator)\n",
    "- [a training routine](#Training)\n",
    "- \\+ [application exercises](#Application-exercise)\n",
    "\n",
    "Hopefully by the end of this tutorial you will have an understanding of the building blocks needed for training (deep) neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreword\n",
    "\n",
    "We will purely rely on numpy for this tutorial. Make sure to import it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Oriented Python\n",
    "\n",
    "Object-oriented Python, a.k.a _classes_, will be used in this tutorial.  \n",
    "For those not familiar with Python classes, know that you will only be required to write some definitions and Python code **within the** class **methods** and **not** actually **write any class**.  \n",
    "\n",
    "If you want to know more about Python classes, here is a step by step [tutorial](https://aboucaud.github.io/slides/2016/python-classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "A loss function measures how good our predictions are, compared to the expected values. It is the cost function that needs to be minimised. The loss function must be differentiable, as its gradient is needed to adjust the parameters of the network through backpropagation.\n",
    "\n",
    "Below is generic loss class. It implements \n",
    "- `loss()` : **the loss** computated from the expected label and the predicted one,\n",
    "- `grad()` : **the gradient of the loss**, needed for the backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #1 - mean square error loss\n",
    "\n",
    "***5 min*** - *Implement the `MeanSquareError` class*\n",
    "\n",
    "In this exercise, `predicted` and `actual` are vectors (1-d numpy arrays).  \n",
    "You must implement both the loss and its derivative using these two vectors.\n",
    "\n",
    "For info, the mean square error loss function is defined as\n",
    "\n",
    "$${\\rm loss_{MSE}}(y_{true}, y_{pred}) = \\sum \\left(y_{pred} - y_{true}\\right) ^ 2$$\n",
    "and its gradient\n",
    "$$\\nabla {\\rm loss_{MSE}}(y_{true}, y_{pred}) = 2 \\cdot (y_{pred} - y_{true})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquareError:\n",
    "    def loss(self, y_predicted, y_true):\n",
    "        return np.sum((y_predicted - y_true) ** 2)\n",
    "    \n",
    "    def grad(self, y_predicted, y_true):\n",
    "        return 2 * (y_predicted - y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "\n",
    "### Neuron\n",
    "\n",
    "A neuron $\\mathscr{N}$ has multiple input values (vector $\\mathbf{x}$ of size $m$) and a single output $z$. Each neuron is characterised by its weights (vector $\\mathbf{w}$ of size $m$) and a constant bias $b$ to perform the linear operation\n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\mathscr{N}_{\\mathbf{w}, b}(\\mathbf{x}) &= \\sum_i w_i.x_i + b \\\\\n",
    "                                   &= \\begin{bmatrix} w_{0} & \\cdots & w_{m}\\end{bmatrix} \\begin{bmatrix} x_0 \\\\ \\vdots \\\\ x_m \\end{bmatrix} + b\\\\\n",
    "                                   &= \\mathbf{w}^T\\mathbf{x} + b \\\\\n",
    "                                   &= z\n",
    "\\end{aligned}$$\n",
    "where $m$ is the input size, and output a single value $z$.\n",
    "\n",
    "\n",
    "### Linear layer\n",
    "\n",
    "A linear layer $\\mathscr{L}$ is a set of neurons, and can therefore be represented by a matrix of weights $\\mathbf{W}$ and a vector of constants $\\mathbf{b}$.  \n",
    "For a layer of $n$ neurons, the matrix $\\mathbf{W}$ is therefore $(m,n)$ and the vector $\\mathbf{b}$ is of size $n$.  \n",
    "\n",
    "The operation realized by the layer on the input vector $\\mathbf{x}$ of size $m$ is\n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\mathscr{L}_{\\mathbf{W}, \\mathbf{b}}(\\mathbf{x}) \n",
    "% \\begin{bmatrix} y_0 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\n",
    "    &= \\begin{bmatrix} \\sum_i W_{i, 0}.x_i + b_0 \\\\ \\vdots \\\\  \\sum_i W_{i, n}.x_i + b_n \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} W_{0,0} & \\cdots & W_{m,0} \\\\ \\vdots & & \\vdots \\\\ W_{0,n} & \\cdots & W_{m,n} \\\\\\end{bmatrix} . \\begin{bmatrix} x_0 \\\\ \\vdots \\\\ x_m \\end{bmatrix} + \\begin{bmatrix} b_0 \\\\ \\vdots \\\\ b_n \\end{bmatrix} \\\\\n",
    "    &= \\mathbf{W}^T\\mathbf{x} + \\mathbf{b} \\\\\n",
    "    &= \\mathbf{z}\n",
    "\\end{aligned}$$\n",
    "\n",
    "which is a matrix multiplication and an addition, that produces an output vector $\\mathbf{z}$ of size $n$.\n",
    "\n",
    "### Activation layer\n",
    "\n",
    "After the layer forward pass, there might be an activation layer whose role is to break the linearity of the network. The so-called activation layer is thus a non-linear fonction $f$ acting on the output $\\mathbf{z}$ of the linear layer. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{a} &= f(\\mathbf{z}) \\\\\n",
    "           &= f(\\mathbf{W}^T\\mathbf{x} + \\mathbf{b})\n",
    "\\end{aligned}$$\n",
    "\n",
    "The activation layer conserves the shape.\n",
    "\n",
    "### Backpropagation (computing of the layer gradients)\n",
    "\n",
    "For the backward pass, each layer receives a gradient vector for the preceding layer.\n",
    "\n",
    "The ***chain rule*** connects the the loss $\\mathscr{C}$ (for cost) to the weights and biases of layer $i$ and yields the following relations  :\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\dfrac{\\partial \\mathscr{C}}{\\partial \\mathbf{W}^{i}} \n",
    "    &= \\dfrac{\\partial \\mathscr{C}}{\\partial \\mathbf{a}^{i}} \\cdot \\dfrac{\\partial \\mathbf{a}^{i}}{\\partial \\mathbf{z}^{i}} \\cdot \\dfrac{\\partial \\mathbf{z}^{i}}{\\partial \\mathbf{W}^{i}} \\\\\n",
    "    &= \\mathbf{\\nabla}\\mathscr{C}^{i} \\cdot f'(\\mathbf{z}^{i}) \\cdot \\mathbf{x}^{i} \\\\\n",
    "    &= \\mathbf{\\nabla_W^i}\n",
    "\\end{aligned}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\dfrac{\\partial \\mathscr{C}}{\\partial \\mathbf{b}^{i}} \n",
    "    &= \\dfrac{\\partial \\mathscr{C}}{\\partial \\mathbf{a}^{i}} \\cdot \\dfrac{\\partial \\mathbf{a}^{i}}{\\partial \\mathbf{z}^{i}} \\cdot \\dfrac{\\partial \\mathbf{z}^{i}}{\\partial \\mathbf{b}^{i}} \\\\\n",
    "    &= \\mathbf{\\nabla}\\mathscr{C}^{i} \\cdot f'(\\mathbf{z}^{i}) \\\\\n",
    "    &= \\mathbf{\\nabla_b^i}\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $\\mathbf{\\nabla}\\mathscr{C}^{i}$ is the gradient vector of the loss propagated at layer $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we define sequential neural nets, made of one or more layers.\n",
    "The layer will  pass its inputs forward\n",
    "and propagate gradients backward. \n",
    "\n",
    "For example, a neural net might look like `inputs -> Linear -> Tanh -> Linear -> output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base class for a layer has a dictionary to store parameters ($\\mathbf{W}$, $\\mathbf{b}$) and gradients ($\\mathbf{\\nabla_W}$, $\\mathbf{\\nabla_b}$) and implements a forward and a backward method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #2 - linear layer\n",
    "\n",
    "***10 - 15 min*** - *Implement the `forward` and `backward` methods of the linear layer.*\n",
    "\n",
    "The mathematical recap above is here to help you.  \n",
    "\n",
    "Be aware that neural networks are generally trained in batches (see [batch generator part](#Batch-generator)), essentially in order to \n",
    "- save some foward / backward computing steps\n",
    "- reduce the noise produced by extreme input vectors at the optimisation step.\n",
    "\n",
    "We therefore introduce the concept of ***batch_size***, which is the number of simultaneous trained inputs.\n",
    "For this reason, the input and output arrays of the layers are not actual vectors but **matrices** whose shape of one dimension is the ***batch_size***.\n",
    "\n",
    "Hints:\n",
    "- matrix products can be written either with `np.dot(m1, m2)` or `m1 @ m2` with recent Python versions (3.5+)\n",
    "- pay a specific attention to the shape of the input and output matrices for the matrix product\n",
    "- $\\mathbf{W}^T\\mathbf{x}$ is written `x @ W`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    \"\"\"\n",
    "    Inputs are of size (batch_size, input_size)\n",
    "    Outputs are of size (batch_size, output_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "        # Initialize the weights and bias with random values\n",
    "        self.params[\"w\"] = np.random.randn(input_size, output_size)\n",
    "        self.params[\"b\"] = np.random.randn(output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs shape is (batch_size, input_size)\n",
    "        W shape is (input_size, output_size)\n",
    "        b shape is (output_size)\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        W = self.params[\"w\"]\n",
    "        b = self.params[\"b\"]\n",
    "        # Compute here the feed forward pass\n",
    "        return inputs @ W + b\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        \"\"\"\n",
    "        grad shape is (batch_size, output_size)\n",
    "        return shape is (batch_size, input_size)\n",
    "        gradW shape is the same as W shape\n",
    "        gradb shape is the same as b shape\n",
    "        \"\"\"\n",
    "        X = self.inputs\n",
    "        W = self.params[\"w\"]\n",
    "        # Compute here the gradient parameters for the layer\n",
    "        self.grads[\"w\"] = X.T @ grad\n",
    "        self.grads[\"b\"] = np.sum(grad, axis=0)\n",
    "        # Compute here the feed backward pass\n",
    "        return grad @ W.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #3 - tanh\n",
    "\n",
    "***5 min*** - *Implement the hyperbolic tangent and sigmoid layers and their derivatives.*\n",
    "\n",
    "Look for the definitions in the lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.grads = {}\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.tanh(inputs)\n",
    "    \n",
    "    def backward(self, gradients):\n",
    "        f_prime = 1 - np.tanh(self.inputs) ** 2\n",
    "        return f_prime * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "A neural net is a collection of layers. It takes care of sequentially calling the layers `forward` and a `backward` methods in the right order.\n",
    "\n",
    "In addition, it implements a getter method `params_and_grads` that will be used by the optimizer to update the values of the weights and bias of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def compile(self, loss, optimizer):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        The forward pass takes the layers in order\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backprop(self, grad):\n",
    "        \"\"\"sequential gradient computation and backward pass to the next layer\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "The role of the optimizer is to adjust the network parameters (weights and biases of the linear layers here) based on the gradients computed during backpropagation.\n",
    "\n",
    "The main attribute of an optimizer is the _learning rate_ (a.k.a. `lr`), which defines the size of the jump taken in the direction of the gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #4 - Stochastic Gradient Descent\n",
    "\n",
    "***5 min*** - write the optimizer step\n",
    "\n",
    "Here we have a very basic implementation of a _Stochastic Gradient Descent_ (a.k.a. `SGD`). \n",
    "\n",
    "The step that needs to be written iterates over the neural network layers and updates the layers parameters in the direction _opposite_ to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01) -> None:\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, layers):\n",
    "        for layer in layers:\n",
    "            for name, param in layer.params.items():\n",
    "                grad = layer.grads[name]\n",
    "                param -= self.lr * grad\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add update method to the NeuralNet class to take a step\n",
    "def update(self):\n",
    "    self.optimizer.step(self.layers)               \n",
    "\n",
    "NeuralNet.update = update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generator\n",
    "\n",
    "It can be costly to compute the gradients and update the weights after every entry of the training dataset. In order to minimize such computational cost, the inputs of the network are traditionally fed in batches and the gradients are thus averages over those batches of data.\n",
    "\n",
    "A batch size of 32 is a default in multiple training sets. Some recent [study](https://arxiv.org/abs/1804.07612) claims this number is the perfect balance between computing efficiency and training stability.\n",
    "\n",
    "During an epoch the network will iterate over the whole dataset. Adding some shuffling in the process ensures the batches are not fed exactly in the same order at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchIterator:\n",
    "    def __init__(self, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __call__(self, inputs, targets):\n",
    "        starts = np.arange(0, len(inputs), self.batch_size)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(starts)\n",
    "\n",
    "        for start in starts:\n",
    "            end = start + self.batch_size\n",
    "            batch_inputs = inputs[start:end]\n",
    "            batch_targets = targets[start:end]\n",
    "            yield batch_inputs, batch_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training routine uses all objects defined above and executes actions **in the right order** to train the neural network.\n",
    "\n",
    "The dataset being usually small with respect to the number of free parameters of the neural net, going through the dataset multiple times during the training is a necessity. This corresponds to the number of epochs, which has to be specified.\n",
    "\n",
    "### Exercise #5 - build the training routine\n",
    "\n",
    "***10 min*** - write the sequential steps needed for training at each epoch\n",
    "\n",
    "_Hints_:\n",
    "- feed forward\n",
    "- compute the loss and the gradients\n",
    "- feed backwards\n",
    "- update the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, inputs, targets, batch_size=32, epochs=2000):\n",
    "    iterator = BatchIterator(batch_size=batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for (batch_inputs, batch_targets) in iterator(inputs, targets):\n",
    "            X = batch_inputs\n",
    "            y_true = batch_targets\n",
    "            # Compute the predictions of the current network\n",
    "            y_predicted = self.predict(X)\n",
    "            # Compute the loss\n",
    "            epoch_loss += self.loss.loss(y_predicted, y_true)\n",
    "            # Compute the gradient of the loss\n",
    "            grad = self.loss.grad(y_predicted, y_true)\n",
    "            # Backpropagate the gradients\n",
    "            self.backprop(grad)\n",
    "            # Update the network\n",
    "            self.update()\n",
    "            \n",
    "        # Print status every 100 iterations\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the function to the NeuralNet class\n",
    "NeuralNet.fit = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application exercise\n",
    "\n",
    "Now that you have build your own neural network library, let's use it to solve a problem and then put it in application.\n",
    "\n",
    "### XOR\n",
    "\n",
    "Canonical problem in ML as there is not linear way to map the inputs to the output.\n",
    "\n",
    "```\n",
    "[0, 0] => 0  \n",
    "[0, 1] => 1  \n",
    "[1, 0] => 1  \n",
    "[1, 1] => 0  \n",
    "```\n",
    "\n",
    "Because of the extremely small size of the dataset, we will **forget** about the prescriptions on _training, validation and test sets_ for this example, which **you shouldn't do in practice**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_xor_results(net, inputs, targets):\n",
    "    predictions = net.predict(inputs)\n",
    "    print('\\nX => y => y_pred => round(y_pred)')\n",
    "    for a, b, c in zip(inputs, targets, predictions):\n",
    "        print(f'{a} => {b} => {c} => {c.round()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help visualise the final decisions to which the network as converge, the decision contours can be drown from it using a grid of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_decision_contours(network, bounds=[0, 1, 0, 1]):\n",
    "    # Create an array of points to plot the decision regions\n",
    "    x_min, x_max, y_min, y_max = bounds\n",
    "    rows, cols = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    X_grid = np.c_[rows.ravel(), cols.ravel()]\n",
    "    # Apply the decision function on the two vectors\n",
    "    values = network.predict(X_grid)\n",
    "    # Reshape the array to recover the squared shape\n",
    "    values = values.reshape(rows.shape)\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    # Plot decision region\n",
    "    plt.pcolormesh(rows, cols, values > 0.5, \n",
    "                   cmap='Paired')\n",
    "    plt.grid(False)\n",
    "    # Plot decision boundaries\n",
    "    plt.contour(rows, cols, values, \n",
    "                levels=[.25, .5, .75],\n",
    "                colors=['k', 'k', 'k'], \n",
    "                linestyles=['--', '-', '--'])\n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an attempt at solving the XOR problem using a single linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss and optimizer\n",
    "sgd = SGD(lr=0.05)\n",
    "mse = MeanSquareError()\n",
    "\n",
    "# Create empty neural network\n",
    "net1 = NeuralNet()\n",
    "# Add layers\n",
    "net1.add(LinearLayer(input_size=2, output_size=1))\n",
    "# Add loss and optimizer\n",
    "net1.compile(loss=mse, optimizer=sgd)\n",
    "# Train the model\n",
    "net1.fit(X, y, batch_size=32, epochs=2000)\n",
    "\n",
    "print_xor_results(net1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single linear layer does not work, as expected. XOR is a typical non-linear problem.\n",
    "\n",
    "Let's have a look at the decision contours of the optimised net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #6 - solve XOR with a Neural Net\n",
    "\n",
    "***5 min*** - Write a more advanced neural net (using additional linear and activation layers) until the predictions match the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01)\n",
    "mse = MeanSquareError()\n",
    "\n",
    "net2 = NeuralNet()\n",
    "net2.add(LinearLayer(input_size=2, output_size=4))\n",
    "net2.add(Tanh())\n",
    "net2.add(LinearLayer(input_size=4, output_size=1))\n",
    "\n",
    "net2.compile(loss=mse, optimizer=sgd)\n",
    "\n",
    "net2.fit(X, y, batch_size=32, epochs=2000)\n",
    "\n",
    "print_xor_results(net2, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_contours(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #7 - write the same model using Keras\n",
    "\n",
    "***10 min*** - Based on the `Keras` examples given in the [lecture](https://aboucaud.github.io/slides/2019/neural-networks-asterics), as well as the section on loss and optimizers, solve the XOR problem using `Keras` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down the keras model below\n",
    "#---------------------------------\n",
    "# Star by the necessary imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Write the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=2, activation='tanh'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# Train the model (no validation_split required here)\n",
    "model.fit(X, y, epochs=2000, verbose=0)\n",
    "#---------------------------------\n",
    "\n",
    "# Once trained, this will then predict the values (equivalent of `.forward()`)\n",
    "y_pred_keras = model.predict(X)\n",
    "\n",
    "# And print the results\n",
    "print_xor_results(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_contours(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "The idea and the code for this tutorial have been for the most part inspired by the video \"Deep Learning Madness\" https://youtu.be/o64FV-ez6Gw by [Joel Grus](https://twitter.com/joelgrus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
